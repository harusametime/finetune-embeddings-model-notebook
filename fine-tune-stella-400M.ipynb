{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbe2c0a-3a28-4f10-b570-2d3387aa8f69",
   "metadata": {},
   "source": [
    "# Fine-tuning embeddings model\n",
    "\n",
    "## Installing library\n",
    "\n",
    "PyTorch 2.5.1, TorchVision 0.20, and accelerate 0.26.0 are needed for sentence_tranformers and xformers. Need to run `pip install -U torch torchvision` first.\n",
    "\n",
    "When you see the accelerate version error even if you installed `accelerate==0.26.0`, **please go to `kernel` on the menu of this notebook and run `Restart Kernel`, which resets the library import with new installed versions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e111a-030d-4a6e-b2b5-1642fdba1129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install accelerate==0.26.0\n",
    "!pip install -U torch torchvision\n",
    "!pip install transformers[torch]\n",
    "!pip install -U sentence_transformers\n",
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2367f4-7c50-48c3-8ed6-8f2a5b705f30",
   "metadata": {},
   "source": [
    "## Download dataset \n",
    "\n",
    "This notebook uses AmazonQA dataset, https://huggingface.co/datasets/embedding-data/Amazon-QA\n",
    "\n",
    "The dataset consits of over 1M pairs of a question (feature name \"query\") and list of answers (feature name \"pos\"). Fine-tuning embeddings model requires pairs of a question and an answer because text tokenizer assumes the pair. In the following, we apply pre-processing function named `select_one_positive` to extract the first answer of the answer list. We can apply the function using `map` function when `load_dataset`.\n",
    "\n",
    "The dataset is split to training dataset (80%), validation dataset (5%) and test dataset (15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327cba6-3792-4e8a-a629-aa4c7f0bf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Dataset\n",
    "def select_one_positive(example, feature ='pos'):\n",
    "    example[feature] = example[feature][0]\n",
    "    return example\n",
    "\n",
    "dataset_name = \"embedding-data/Amazon-QA\"\n",
    "train_dataset = load_dataset(dataset_name, split='train[:80%]').map(select_one_positive)\n",
    "train_dataset.info.dataset_name = dataset_name +\"_train\"\n",
    "val_dataset = load_dataset(dataset_name, split='train[80%:85%]').map(select_one_positive)\n",
    "val_dataset.info.dataset_name = dataset_name +\"_val\"\n",
    "test_dataset = load_dataset(dataset_name, split='train[85%:]').map(select_one_positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d2f16e-405d-45f8-92ad-c8e1a0c769bc",
   "metadata": {},
   "source": [
    "## Fine-tuning script\n",
    "\n",
    "This notebooks uses the model named \"dunzhang/stella_en_400M_v5\". We can use other huggingface model, which is compatible with SentenceTransformer library. This takes 8-9 hours with ml.g5.16xlarge (Single A10 GPU instance). Training loss is outputted every 1000 training steps (when feeding 8000 pairs = 1000steps X 8 pairs in one batch), and checkpoint is saved every 2000 training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a281c-25ad-437a-905f-923b448beb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1968' max='6845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1968/6845 2:36:31 < 6:28:16, 0.21 it/s, Epoch 0.29/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"./output\"\n",
    "n_epochs = 1\n",
    "batch_size = 8\n",
    "patience = 2\n",
    "checkpoint = True\n",
    "train_files = 1000\n",
    "model_name = \"dunzhang/stella_en_400M_v5\"\n",
    "\n",
    "\n",
    "# ÔºÅThe default dimension is 1024, if you need other dimensions, please clone the model and modify `modules.json` to replace `2_Dense_1024` with another dimension, e.g. `2_Dense_256` or `2_Dense_8192` !\n",
    "# on gpu\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True, config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False} ).cuda()\n",
    "\n",
    "# Define loss\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Calculate gradient accumulation steps: 128/batch_size, clamped between 1 and 64\n",
    "gradient_accumulation_steps = min(max(128 // batch_size, 1), 64)\n",
    "effective_batch_size = batch_size * gradient_accumulation_steps\n",
    "learning_rate = 2e-5 * effective_batch_size / 64\n",
    "\n",
    "# Training arguments\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=n_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=checkpoint,\n",
    "    eval_strategy=\"no\" if val_dataset is None else \"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=val_dataset is not None,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize trainer with early stopping only if validation is enable\n",
    "trainer_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"args\": training_args,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"loss\": loss,\n",
    "}\n",
    "if val_dataset is not None:\n",
    "    trainer_kwargs.update({\n",
    "        \"eval_dataset\": val_dataset,\n",
    "        \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    })\n",
    "\n",
    "trainer = SentenceTransformerTrainer(**trainer_kwargs)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model_name = model.split('/')[-1]\n",
    "output_path = os.path.join(dataset_name, f'fine_tuned_{model_name}_{train_files}')\n",
    "model.save(output_path)\n",
    "print(f\"Model saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ee872-d3f3-46b4-9595-781216e1a827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
