{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbe2c0a-3a28-4f10-b570-2d3387aa8f69",
   "metadata": {},
   "source": [
    "# Fine-tuning embeddings model\n",
    "\n",
    "## Installing library\n",
    "\n",
    "PyTorch 2.5.1, TorchVision 0.20, and accelerate 0.26.0 are needed for sentence_tranformers and xformers. Need to run `pip install -U torch torchvision` first.\n",
    "\n",
    "When you see the accelerate version error even if you installed `accelerate==0.26.0`, **please go to `kernel` on the menu of this notebook and run `Restart Kernel`, which resets the library import with new installed versions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e111a-030d-4a6e-b2b5-1642fdba1129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install accelerate==0.26.0\n",
    "!pip install -U torch torchvision datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install -U sentence_transformers\n",
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2367f4-7c50-48c3-8ed6-8f2a5b705f30",
   "metadata": {},
   "source": [
    "## Download dataset \n",
    "\n",
    "This notebook uses AmazonQA dataset, https://huggingface.co/datasets/embedding-data/Amazon-QA\n",
    "\n",
    "The dataset consits of over 1M pairs of a question (feature name \"query\") and list of answers (feature name \"pos\"). Fine-tuning embeddings model requires pairs of a question and an answer because text tokenizer assumes the pair. In the following, we apply pre-processing function named `select_one_positive` to extract the first answer of the answer list. We can apply the function using `map` function when `load_dataset`.\n",
    "\n",
    "The dataset is split to training dataset (80%), validation dataset (5%) and test dataset (15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327cba6-3792-4e8a-a629-aa4c7f0bf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Dataset\n",
    "def select_one_positive(example, feature ='pos'):\n",
    "    example[feature] = example[feature][0]\n",
    "    return example\n",
    "\n",
    "dataset_name = \"embedding-data/Amazon-QA\"\n",
    "train_dataset = load_dataset(dataset_name, split='train[:80%]').map(select_one_positive)\n",
    "# train_dataset.info.dataset_name = dataset_name +\"_train\"\n",
    "val_dataset = load_dataset(dataset_name, split='train[80%:85%]').map(select_one_positive)\n",
    "# val_dataset.info.dataset_name = dataset_name +\"_val\"\n",
    "test_dataset = load_dataset(dataset_name, split='train[85%:]').map(select_one_positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d2f16e-405d-45f8-92ad-c8e1a0c769bc",
   "metadata": {},
   "source": [
    "## Fine-tuning script\n",
    "\n",
    "This notebooks uses the model named \"dunzhang/stella_en_400M_v5\". We can use other huggingface model, which is compatible with SentenceTransformer library. This takes 8-9 hours with ml.g5.16xlarge (Single A10 GPU instance). Training loss is outputted every 1000 training steps (when feeding 8000 pairs = 1000steps X 8 pairs in one batch), and checkpoint is saved every 2000 training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a281c-25ad-437a-905f-923b448beb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./user-default-efs/output\"\n",
    "n_epochs = 1\n",
    "batch_size = 8\n",
    "patience = 2\n",
    "checkpoint = True\n",
    "train_files = 1000\n",
    "model_name = \"dunzhang/stella_en_400M_v5\"\n",
    "\n",
    "\n",
    "# ÔºÅThe default dimension is 1024, if you need other dimensions, please clone the model and modify `modules.json` to replace `2_Dense_1024` with another dimension, e.g. `2_Dense_256` or `2_Dense_8192` !\n",
    "# on gpu\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True, config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False} ).cuda()\n",
    "\n",
    "# Define loss\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Calculate gradient accumulation steps: 128/batch_size, clamped between 1 and 64\n",
    "gradient_accumulation_steps = min(max(128 // batch_size, 1), 64)\n",
    "effective_batch_size = batch_size * gradient_accumulation_steps\n",
    "learning_rate = 2e-5 * effective_batch_size / 64\n",
    "\n",
    "# Training arguments\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=n_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=checkpoint,\n",
    "    eval_strategy=\"no\" if val_dataset is None else \"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=val_dataset is not None,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize trainer with early stopping only if validation is enable\n",
    "trainer_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"args\": training_args,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"loss\": loss,\n",
    "}\n",
    "if val_dataset is not None:\n",
    "    trainer_kwargs.update({\n",
    "        \"eval_dataset\": val_dataset,\n",
    "        \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    })\n",
    "\n",
    "trainer = SentenceTransformerTrainer(**trainer_kwargs)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "last_model_name = model_name.split('/')[-1]\n",
    "output_path = os.path.join(output_dir, dataset_name, f'fine_tuned_{last_model_name}')\n",
    "model.save(output_path)\n",
    "print(f\"Model saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34793ac9-a895-49f2-a3a3-082f34e318b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0988</td>\n",
       "      <td>40.514362</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.073040</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0933</td>\n",
       "      <td>43.092133</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.146080</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.146080</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.085428</td>\n",
       "      <td>491.5249</td>\n",
       "      <td>111.417</td>\n",
       "      <td>13.928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0917</td>\n",
       "      <td>52.983276</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.219120</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0821</td>\n",
       "      <td>32.844742</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.292160</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.292160</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.077204</td>\n",
       "      <td>491.4890</td>\n",
       "      <td>111.425</td>\n",
       "      <td>13.929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0768</td>\n",
       "      <td>24.372654</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0736</td>\n",
       "      <td>43.456551</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.438240</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.438240</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.072186</td>\n",
       "      <td>492.9578</td>\n",
       "      <td>111.093</td>\n",
       "      <td>13.888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0722</td>\n",
       "      <td>29.331873</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.511280</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0690</td>\n",
       "      <td>38.391743</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.584320</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.584320</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.061152</td>\n",
       "      <td>492.9690</td>\n",
       "      <td>111.090</td>\n",
       "      <td>13.887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0623</td>\n",
       "      <td>32.878979</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.657360</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0597</td>\n",
       "      <td>30.439440</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.730400</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.730400</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.053444</td>\n",
       "      <td>492.4807</td>\n",
       "      <td>111.200</td>\n",
       "      <td>13.901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0558</td>\n",
       "      <td>30.118591</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.803440</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0533</td>\n",
       "      <td>32.484440</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.876480</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.876480</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.048777</td>\n",
       "      <td>491.9017</td>\n",
       "      <td>111.331</td>\n",
       "      <td>13.917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>46.404884</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.949520</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>6845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34363.5066</td>\n",
       "      <td>25.499</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.0988  40.514362       0.000029  0.073040   500        NaN           NaN   \n",
       "1   0.0933  43.092133       0.000038  0.146080  1000        NaN           NaN   \n",
       "2      NaN        NaN            NaN  0.146080  1000   0.085428      491.5249   \n",
       "3   0.0917  52.983276       0.000035  0.219120  1500        NaN           NaN   \n",
       "4   0.0821  32.844742       0.000031  0.292160  2000        NaN           NaN   \n",
       "5      NaN        NaN            NaN  0.292160  2000   0.077204      491.4890   \n",
       "6   0.0768  24.372654       0.000028  0.365200  2500        NaN           NaN   \n",
       "7   0.0736  43.456551       0.000025  0.438240  3000        NaN           NaN   \n",
       "8      NaN        NaN            NaN  0.438240  3000   0.072186      492.9578   \n",
       "9   0.0722  29.331873       0.000022  0.511280  3500        NaN           NaN   \n",
       "10  0.0690  38.391743       0.000018  0.584320  4000        NaN           NaN   \n",
       "11     NaN        NaN            NaN  0.584320  4000   0.061152      492.9690   \n",
       "12  0.0623  32.878979       0.000015  0.657360  4500        NaN           NaN   \n",
       "13  0.0597  30.439440       0.000012  0.730400  5000        NaN           NaN   \n",
       "14     NaN        NaN            NaN  0.730400  5000   0.053444      492.4807   \n",
       "15  0.0558  30.118591       0.000009  0.803440  5500        NaN           NaN   \n",
       "16  0.0533  32.484440       0.000005  0.876480  6000        NaN           NaN   \n",
       "17     NaN        NaN            NaN  0.876480  6000   0.048777      491.9017   \n",
       "18  0.0519  46.404884       0.000002  0.949520  6500        NaN           NaN   \n",
       "19     NaN        NaN            NaN  0.999918  6845        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                   111.417                 13.928            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                   111.425                 13.929            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                   111.093                 13.888            NaN   \n",
       "9                       NaN                    NaN            NaN   \n",
       "10                      NaN                    NaN            NaN   \n",
       "11                  111.090                 13.887            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                  111.200                 13.901            NaN   \n",
       "15                      NaN                    NaN            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                  111.331                 13.917            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                      NaN                    NaN     34363.5066   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
       "0                        NaN                     NaN         NaN         NaN  \n",
       "1                        NaN                     NaN         NaN         NaN  \n",
       "2                        NaN                     NaN         NaN         NaN  \n",
       "3                        NaN                     NaN         NaN         NaN  \n",
       "4                        NaN                     NaN         NaN         NaN  \n",
       "5                        NaN                     NaN         NaN         NaN  \n",
       "6                        NaN                     NaN         NaN         NaN  \n",
       "7                        NaN                     NaN         NaN         NaN  \n",
       "8                        NaN                     NaN         NaN         NaN  \n",
       "9                        NaN                     NaN         NaN         NaN  \n",
       "10                       NaN                     NaN         NaN         NaN  \n",
       "11                       NaN                     NaN         NaN         NaN  \n",
       "12                       NaN                     NaN         NaN         NaN  \n",
       "13                       NaN                     NaN         NaN         NaN  \n",
       "14                       NaN                     NaN         NaN         NaN  \n",
       "15                       NaN                     NaN         NaN         NaN  \n",
       "16                       NaN                     NaN         NaN         NaN  \n",
       "17                       NaN                     NaN         NaN         NaN  \n",
       "18                       NaN                     NaN         NaN         NaN  \n",
       "19                    25.499                   0.199         0.0    0.071104  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f831d92-6a48-4c71-9b58-74d0d78de52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
